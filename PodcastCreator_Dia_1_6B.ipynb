{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMV66iDDX1RikVEp6BExaig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff3d098a8b9e4695863c319f085f66b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0274b6f400d04155b1dd0cead5149f7c",
              "IPY_MODEL_31f721d031074d65bacca9500f152840",
              "IPY_MODEL_c4157d36f16d4383a51f949bfa91d748"
            ],
            "layout": "IPY_MODEL_2ede26736f2b42d39ed158abd91777ed"
          }
        },
        "0274b6f400d04155b1dd0cead5149f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_952f4f361f0a41889bde25e09b22b4d0",
            "placeholder": "​",
            "style": "IPY_MODEL_da20ef1db1b14eb6b47e5a2a2f89faa7",
            "value": "config.json: 100%"
          }
        },
        "31f721d031074d65bacca9500f152840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7720bd087dc3445cb23beac8240e724a",
            "max": 941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d543f3e3d11424abe1fa376e131c31f",
            "value": 941
          }
        },
        "c4157d36f16d4383a51f949bfa91d748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eac881755dc047c4b795f78e239d1641",
            "placeholder": "​",
            "style": "IPY_MODEL_640a004128774e0881e4a8f5a346ebc5",
            "value": " 941/941 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "2ede26736f2b42d39ed158abd91777ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "952f4f361f0a41889bde25e09b22b4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da20ef1db1b14eb6b47e5a2a2f89faa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7720bd087dc3445cb23beac8240e724a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d543f3e3d11424abe1fa376e131c31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eac881755dc047c4b795f78e239d1641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "640a004128774e0881e4a8f5a346ebc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3517c930705049c8a2505a3d43608b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff53dd266d2846329ad9d5b7329269d7",
              "IPY_MODEL_8575b742f4244505b1bab7432675bf94",
              "IPY_MODEL_e09bbc9c36194daeb8212ed868e5438a"
            ],
            "layout": "IPY_MODEL_e4824dc237934cf587d66ecb2c325f38"
          }
        },
        "ff53dd266d2846329ad9d5b7329269d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33cf17714d0d4acdac254fd6fb6d3716",
            "placeholder": "​",
            "style": "IPY_MODEL_38ea657e266c4ddeb7c73cfc08e93da7",
            "value": "model.safetensors: 100%"
          }
        },
        "8575b742f4244505b1bab7432675bf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb00373f256e48cda0cfa448f7b1b529",
            "max": 6444682848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93d0433117ab4eeca79c280c2b8f505b",
            "value": 6444682848
          }
        },
        "e09bbc9c36194daeb8212ed868e5438a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a28025a172460ab8c000edecd10cfb",
            "placeholder": "​",
            "style": "IPY_MODEL_a07c86a1c1d74732a05e42ab5b63189e",
            "value": " 6.44G/6.44G [00:31&lt;00:00, 226MB/s]"
          }
        },
        "e4824dc237934cf587d66ecb2c325f38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33cf17714d0d4acdac254fd6fb6d3716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ea657e266c4ddeb7c73cfc08e93da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb00373f256e48cda0cfa448f7b1b529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d0433117ab4eeca79c280c2b8f505b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a28025a172460ab8c000edecd10cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07c86a1c1d74732a05e42ab5b63189e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cburchett/podcastcreator/blob/main/PodcastCreator_Dia_1_6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gradio for the user interface\n",
        "!pip -q install gradio"
      ],
      "metadata": {
        "id": "iMpzAkbKuKQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c821ce2-0cf9-4c2e-92a0-7ff10a38820b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2WZzjZw5JXN",
        "outputId": "7c912fc2-1195-4ddd-c380-25c42b7e63e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nari-tts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for argbind (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for randomname (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-aiplatform 1.90.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-cloud-iam 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-cloud-dataproc 5.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.20.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.8 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.30.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.19.6 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.20.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.31.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-spanner 3.54.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install directly from GitHub\n",
        "!pip -q install git+https://github.com/nari-labs/dia.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained Dia model\n",
        "from dia.model import Dia\n",
        "\n",
        "audio_model = Dia.from_pretrained(\"nari-labs/Dia-1.6B\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "ff3d098a8b9e4695863c319f085f66b6",
            "0274b6f400d04155b1dd0cead5149f7c",
            "31f721d031074d65bacca9500f152840",
            "c4157d36f16d4383a51f949bfa91d748",
            "2ede26736f2b42d39ed158abd91777ed",
            "952f4f361f0a41889bde25e09b22b4d0",
            "da20ef1db1b14eb6b47e5a2a2f89faa7",
            "7720bd087dc3445cb23beac8240e724a",
            "5d543f3e3d11424abe1fa376e131c31f",
            "eac881755dc047c4b795f78e239d1641",
            "640a004128774e0881e4a8f5a346ebc5",
            "3517c930705049c8a2505a3d43608b20",
            "ff53dd266d2846329ad9d5b7329269d7",
            "8575b742f4244505b1bab7432675bf94",
            "e09bbc9c36194daeb8212ed868e5438a",
            "e4824dc237934cf587d66ecb2c325f38",
            "33cf17714d0d4acdac254fd6fb6d3716",
            "38ea657e266c4ddeb7c73cfc08e93da7",
            "eb00373f256e48cda0cfa448f7b1b529",
            "93d0433117ab4eeca79c280c2b8f505b",
            "29a28025a172460ab8c000edecd10cfb",
            "a07c86a1c1d74732a05e42ab5b63189e"
          ]
        },
        "id": "Fq-JBUlGxb5n",
        "outputId": "617de686-894d-462c-9a29-1fa66e3df5e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/941 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff3d098a8b9e4695863c319f085f66b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.44G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3517c930705049c8a2505a3d43608b20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Google API key from userdata and set it as environment variable\n",
        "\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "\n",
        "# Create a client for Google Cloud AI API\n",
        "client = genai.Client(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "iVs9W87Xbk6L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define YouTube URL\n",
        "YOUTUBE_URL = \"https://www.youtube.com/watch?v=rSCaiHFRx0k\""
      ],
      "metadata": {
        "id": "Vzk684GblFZG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt for content generation\n",
        "PROMPT = \"\"\"Analyze the attached Youtube video.\n",
        "\n",
        "Based on the key topics, information, and events presented in the video, generate a medium length, conversational podcast script between two speakers, labeled S1 and S2.\n",
        "\n",
        "The script should summarize or discuss the main points of the video in a natural, back-and-forth dialogue format.\n",
        "\n",
        "**Crucially, format the output *exactly* as follows:**\n",
        "\n",
        "*   Each line of dialogue must start with either `[S1]` or `[S2]`.\n",
        "*   Follow the speaker tag with a space, then their dialogue.\n",
        "*   Present the dialogue turns sequentially, mimicking a conversation.\n",
        "*   Don't add any prefix or suffix to the conversation\n",
        "\n",
        "**Use this specific structure as your template:**\n",
        "\n",
        "```\n",
        "[S1] {Dialogue for speaker 1}\n",
        "[S2] {Dialogue for speaker 2}\n",
        "[S1] {Dialogue for speaker 1, potentially a reaction or follow-up}\n",
        "[S2] {Dialogue for speaker 2}\n",
        "[S1] {Dialogue for speaker 1}\n",
        "```\n",
        "\n",
        "**Example of the desired output format:**\n",
        "\n",
        "```\n",
        "[S1] Hey Sam, How are you? Let me tell you about Dia it's an open weights text to dialogue model.\n",
        "[S2] You get full control over scripts and voices.\n",
        "[S1] Wow. Amazing. (laughs)\n",
        "[S2] Try it now on Git hub or Hugging Face.\n",
        "[S1] You bet I will!\n",
        "```\n",
        "\n",
        "**Constraints:**\n",
        "\n",
        "*   Keep the turns relatively short and conversational.\n",
        "*   Focus on the core message or interesting aspects of the video.\n",
        "*   Adhere strictly to the `[S1]` / `[S2]` formatting.\n",
        "*   **Incorporate non-verbal cues where natural and appropriate.** These should be enclosed in parentheses within the dialogue line (e.g., `(laughs)` or `(sighs)`). You may use cues from this list: `(laughs)`, `(clears throat)`, `(sighs)`, `(gasps)`, `(coughs)`, `(singing)`, `(sings)`, `(mumbles)`, `(beep)`, `(groans)`, `(sniffs)`, `(claps)`, `(screams)`, `(inhales)`, `(exhales)`, `(applause)`, `(burps)`, `(humming)`, `(sneezes)`, `(whistles)`.\n",
        "*   Do not add any introductory text, explanations, or summaries outside of the formatted script itself.\n",
        "\n",
        "**Now, analyze the video and generate the script.**\n",
        "\n",
        "---\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "STB_P4fePBOE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a new folder with a timestamp\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def ensure_folder_exists():\n",
        "    now = datetime.now()\n",
        "    timestamp = now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    folder_path = '/content/' + timestamp\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Folder '{folder_path}' created.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_path}' already exists.\")\n",
        "    return folder_path"
      ],
      "metadata": {
        "id": "xKwJ6GHeneo9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate podcast script using Google Cloud AI API\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "def generate_podcast_script(youtube_url, model, prompt):\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=types.Content(\n",
        "            parts=[\n",
        "                types.Part(text=prompt),\n",
        "                types.Part(\n",
        "                    file_data=types.FileData(file_uri=youtube_url)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "rYVTfmG3octh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split podcast transcript into segments\n",
        "\n",
        "import re\n",
        "\n",
        "def split_podcast_transcript(transcript, pairs):\n",
        "    \"\"\"Splits a podcast transcript into segments based on S1 and S2 pairs.\n",
        "\n",
        "    Args:\n",
        "        transcript: The podcast transcript as a string.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string represents a segment of the transcript.\n",
        "        Returns an empty list if the input is invalid or no valid segments are found.\n",
        "    \"\"\"\n",
        "\n",
        "    segments = []\n",
        "    try:\n",
        "        # Split the transcript into lines\n",
        "        lines = transcript.strip().split('\\n')\n",
        "\n",
        "        # Use regular expressions to find S1 and S2 pairs\n",
        "        pattern = r\"\\[(S[12])\\](.*)\"\n",
        "        s1_s2_pairs = []\n",
        "        for line in lines:\n",
        "          match = re.match(pattern, line)\n",
        "          if match:\n",
        "            s1_s2_pairs.append(match.groups())\n",
        "\n",
        "        for i in range(0, len(s1_s2_pairs), pairs):\n",
        "            segment = \"\"\n",
        "            for j in range(i, min(i + pairs, len(s1_s2_pairs))):\n",
        "                segment += f\"[{s1_s2_pairs[j][0]}] {s1_s2_pairs[j][1]}\\n\"\n",
        "            segments.append(segment.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing transcript: {e}\")\n",
        "        return []\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "QhtHdTi4q3B3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract podcast numbering from filename\n",
        "# Function to combine multiple MP3 files into one\n",
        "import os\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def extract_podcast_numbering(filename):\n",
        "    # Extract the numeric part at the beginning of the filename\n",
        "    match = re.match(r'podcast_(\\d+)\\.mp3', filename)\n",
        "    return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "def combine_mp3s(folder_path, output_file):\n",
        "    \"\"\"Combines all MP3 files in a folder into a single MP3 file.\n",
        "\n",
        "    Args:\n",
        "        folder_path: The path to the folder containing the MP3 files.\n",
        "        output_file: The path to the output MP3 file.\n",
        "    \"\"\"\n",
        "    combined = AudioSegment.empty()\n",
        "    file_list = [f for f in os.listdir(folder_path) if f.endswith(\".mp3\")]\n",
        "    file_list.sort(key=extract_podcast_numbering)\n",
        "    print(\"Sort Order:\", file_list)\n",
        "    for filename in file_list:\n",
        "        if filename.endswith(\".mp3\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                segment = AudioSegment.from_mp3(filepath)\n",
        "                combined += segment\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "    combined.export(output_file, format=\"mp3\")\n",
        "    print(f\"Combined {len(file_list)} MP3 files into {output_file}\")\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "VI4Ne5VSbzti"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_formatted_script_to_file(script_content, output_file_path):\n",
        "    \"\"\"\n",
        "    Writes a formatted Python script to a file for clear reading.\n",
        "\n",
        "    Args:\n",
        "        script_content (str): The content of the Python script as a string.\n",
        "        output_file_path (str): The path to the output file where the script will be written.\n",
        "    \"\"\"\n",
        "    # Split the script content into lines\n",
        "    lines = script_content.split('\\n')\n",
        "\n",
        "    # Format the lines for clear reading\n",
        "    formatted_lines = []\n",
        "    for line in lines:\n",
        "        # Strip leading and trailing whitespace\n",
        "        stripped_line = line.strip()\n",
        "        # Add the formatted line to the list, ensuring proper indentation\n",
        "        if stripped_line:\n",
        "            formatted_lines.append(stripped_line)\n",
        "\n",
        "    # Join the formatted lines back into a single string with proper line breaks\n",
        "    formatted_script = '\\n'.join(formatted_lines)\n",
        "\n",
        "    # Write the formatted script to the output file\n",
        "    with open(output_file_path, 'w') as file:\n",
        "        file.write(formatted_script)\n",
        "\n",
        "    print(f\"Formatted script has been written to {output_file_path}\")"
      ],
      "metadata": {
        "id": "BQ1roOdi1pbE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to set random seed for reproducibility\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "JTDNMITltG4-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate podcast audio\n",
        "\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import time\n",
        "\n",
        "def generate_podcast(\n",
        "      transcript: str,\n",
        "      pairs: int,\n",
        "      max_new_tokens: int,\n",
        "      cfg_scale: float,\n",
        "      temperature: float,\n",
        "      top_p: float,\n",
        "      cfg_filter_top_k: int,\n",
        "      speed_factor: float,\n",
        "      seed: int = 0,\n",
        "    ):\n",
        "\n",
        "    # Create a new folder to store audio segments\n",
        "    folder_path = ensure_folder_exists()\n",
        "\n",
        "    # Set and Display Generation Seed\n",
        "    if seed is None or seed < 0:\n",
        "        seed = random.randint(0, 2 ** 32 - 1)\n",
        "        print(f\"\\nNo seed provided, generated random seed: {seed}\\n\")\n",
        "    else:\n",
        "        print(f\"\\nUsing user-selected seed: {seed}\\n\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Print out transcript\n",
        "    output_file_path = folder_path + \"/final_transcript.txt\"\n",
        "    write_formatted_script_to_file(transcript, output_file_path)\n",
        "    print(f\"Transcript written to {output_file_path}\")\n",
        "\n",
        "    # Split transcript into segments\n",
        "    segments = split_podcast_transcript(transcript, pairs)\n",
        "    print(f\"Number of segments: \" + str(len(segments)))\n",
        "\n",
        "    # Generate audio for each segment\n",
        "    for idx, seg in enumerate(segments):\n",
        "      print(f\"Generating segment {idx+1}\")\n",
        "      print(seg)\n",
        "      start_time = time.time()\n",
        "      # Use torch.inference_mode() context manager for the generation call\n",
        "      with torch.inference_mode():\n",
        "        output = audio_model.generate(\n",
        "            text=seg,\n",
        "            max_tokens=max_new_tokens,\n",
        "            cfg_scale=cfg_scale,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            cfg_filter_top_k=cfg_filter_top_k\n",
        "          )\n",
        "      sf.write(folder_path + f\"/podcast_{idx+1}.mp3\", output, 44100)\n",
        "      end_time = time.time()\n",
        "      print(f\"Generation finished in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "    # Combine audio segments into final podcast\n",
        "    return combine_mp3s(folder_path, folder_path + \"/final_podcast.mp3\")\n"
      ],
      "metadata": {
        "id": "Vgkn8Olhwunl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "ZaC9AnRRliXm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface for podcast generation\n",
        "\n",
        "with gr.Blocks() as podcast_script_generator:\n",
        "  gr.Markdown(\"## Podcast Generator\")\n",
        "  with gr.Tab(\"Script\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        youtube_url = gr.Textbox(label=\"URL\", value=YOUTUBE_URL)\n",
        "        selected_model = gr.Dropdown([\"gemini-2.5-pro-exp-03-25\"])\n",
        "        system_prompt = gr.Textbox(label=\"System prompt\", value=PROMPT)\n",
        "        generate_script_button = gr.Button(\"Generate script\")\n",
        "      with gr.Column():\n",
        "        podcast_script = gr.Textbox(label=\"Podcast script\", lines=20)\n",
        "  generate_script_button.click(fn=generate_podcast_script, inputs=[youtube_url, selected_model, system_prompt], outputs=[podcast_script])\n",
        "\n",
        "  with gr.Tab(\"Podcast\"):\n",
        "     with gr.Row():\n",
        "       with gr.Column():\n",
        "          final_podcast_script = gr.Textbox(label=\"Final podcast script\", lines=20)\n",
        "          pairs = gr.Slider(\n",
        "                    label=\"Segment pairs\",\n",
        "                    minimum=1,\n",
        "                    maximum=10,\n",
        "                    value=1,  # Default\n",
        "                    step=1,\n",
        "                    info=\"Higher values increase number of [S1] [S2] pairs will be in each batch.\",\n",
        "                )\n",
        "          with gr.Accordion(\"Generation Parameters\", open=False):\n",
        "                max_new_tokens = gr.Slider(\n",
        "                    label=\"Max New Tokens (Audio Length)\",\n",
        "                    minimum=860,\n",
        "                    maximum=3072,\n",
        "                    value=audio_model.config.data.audio_length,  # Use config default if available, else fallback\n",
        "                    step=50,\n",
        "                    info=\"Controls the maximum length of the generated audio (more tokens = longer audio).\",\n",
        "                )\n",
        "                cfg_scale = gr.Slider(\n",
        "                    label=\"CFG Scale (Guidance Strength)\",\n",
        "                    minimum=1.0,\n",
        "                    maximum=5.0,\n",
        "                    value=3.0,  # Default\n",
        "                    step=0.1,\n",
        "                    info=\"Higher values increase adherence to the text prompt.\",\n",
        "                )\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature (Randomness)\",\n",
        "                    minimum=1.0,\n",
        "                    maximum=1.5,\n",
        "                    value=1.1,  # Default\n",
        "                    step=0.05,\n",
        "                    info=\"Lower values make the output more deterministic, higher values increase randomness.\",\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top P (Nucleus Sampling)\",\n",
        "                    minimum=0.80,\n",
        "                    maximum=1.0,\n",
        "                    value=0.95,  # Default\n",
        "                    step=0.01,\n",
        "                    info=\"Filters vocabulary to the most likely tokens cumulatively reaching probability P.\",\n",
        "                )\n",
        "                cfg_filter_top_k = gr.Slider(\n",
        "                    label=\"CFG Filter Top K\",\n",
        "                    minimum=15,\n",
        "                    maximum=50,\n",
        "                    value=30,\n",
        "                    step=1,\n",
        "                    info=\"Top k filter for CFG guidance.\",\n",
        "                )\n",
        "                speed_factor_slider = gr.Slider(\n",
        "                    label=\"Speed Factor\",\n",
        "                    minimum=0.8,\n",
        "                    maximum=1.0,\n",
        "                    value=0.84,\n",
        "                    step=0.02,\n",
        "                    info=\"Adjusts the speed of the generated audio (1.0 = original speed).\",\n",
        "                )\n",
        "                seed_input = gr.Number(\n",
        "                    label=\"Generation Seed (Optional)\",\n",
        "                    value=-1,\n",
        "                    precision=0,  # No decimal points\n",
        "                    step=1,\n",
        "                    interactive=True,\n",
        "                    info=\"Set a generation seed for reproducible outputs. Leave empty or -1 for random seed.\",\n",
        "                )\n",
        "          generate_podcast_button = gr.Button(\"Generate podcast\")\n",
        "       with gr.Column():\n",
        "          podcast_audio = gr.Audio(label=\"Final podcast audio\", type='filepath')\n",
        "\n",
        "  generate_podcast_button.click(fn=generate_podcast, inputs=[\n",
        "            final_podcast_script,\n",
        "            pairs,\n",
        "            max_new_tokens,\n",
        "            cfg_scale,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            cfg_filter_top_k,\n",
        "            speed_factor_slider,\n",
        "            seed_input,\n",
        "            ], outputs=[podcast_audio])\n",
        "  podcast_script.change(fn=lambda x: x, inputs=podcast_script, outputs=final_podcast_script)\n",
        "\n",
        "podcast_script_generator.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ce_5emyBljKl",
        "outputId": "ee7732dd-0520-467a-8c16-4337e182be1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e145f779455dee6d27.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e145f779455dee6d27.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/2025-05-04 19:07:47' created.\n",
            "\n",
            "No seed provided, generated random seed: 528958028\n",
            "\n",
            "Formatted script has been written to /content/2025-05-04 19:07:47/transcript.txt\n",
            "Transcript written to /content/2025-05-04 19:07:47/transcript.txt\n",
            "Number of segments: 16\n",
            "Generating segment 1\n",
            "[S1]  So I watched this NetApp video about data infrastructure for AI, and it really highlights how demanding these new workloads are.\n",
            "Generation finished in 53.88 seconds.\n",
            "\n",
            "Generating segment 2\n",
            "[S2]  Yeah, they mentioned AI data pipelines are putting serious pressure on storage architectures, right? The requirements are constantly evolving.\n",
            "Generation finished in 34.15 seconds.\n",
            "\n",
            "Generating segment 3\n",
            "[S1]  Exactly. Especially for things like AI model training, which needs absolutely massive performance.\n",
            "Generation finished in 28.31 seconds.\n",
            "\n",
            "Generating segment 4\n",
            "[S2]  And Krish made a great point – it’s not just raw performance, but getting that performance over standard, familiar protocols like NFS or S3, using the Ethernet networks IT teams already have. (clears throat) No need to rip and replace everything.\n",
            "Generation finished in 60.95 seconds.\n",
            "\n",
            "Generating segment 5\n",
            "[S1]  That makes sense. Plus, the need for flexibility. The infrastructure has to handle these huge training jobs but also scale efficiently for inference later on.\n",
            "Generation finished in 44.38 seconds.\n",
            "\n",
            "Generating segment 6\n",
            "[S2]  And scale *independently*! They talked about scaling storage, compute, and networking separately to really maximise how resources are used.\n",
            "Generation finished in 38.43 seconds.\n",
            "\n",
            "Generating segment 7\n",
            "[S1]  That ties into what Russell was saying too, about fitting AI into the *existing* data center. (laughs) You can't just build new ones overnight.\n",
            "Generation finished in 38.29 seconds.\n",
            "\n",
            "Generating segment 8\n",
            "[S2]  Right! So the underlying storage needs to be super dense and energy-efficient to work within current power and space limits. It has to integrate, not be its own isolated thing.\n",
            "Generation finished in 41.05 seconds.\n",
            "\n",
            "Generating segment 9\n",
            "[S1]  It sounds like IT Ops needs solutions that work with their current tools and skills, rather than something completely alien. AI is becoming just another workload, albeit a demanding one.\n",
            "Generation finished in 40.05 seconds.\n",
            "\n",
            "Generating segment 10\n",
            "[S2]  Absolutely. And it's often hybrid – data and tools might be on-prem *and* in the cloud, maybe even multiple clouds.\n",
            "Generation finished in 36.71 seconds.\n",
            "\n",
            "Generating segment 11\n",
            "[S1]  So you need that seamless experience, right? Moving data easily, managing it consistently wherever it lives, breaking down those silos.\n",
            "Generation finished in 40.00 seconds.\n",
            "\n",
            "Generating segment 12\n",
            "[S2]  Yeah, defying \"data gravity\" as Krish put it. You need that consistent view and operation across your whole environment, on-prem and cloud(s).\n",
            "Generation finished in 37.76 seconds.\n",
            "\n",
            "Generating segment 13\n",
            "[S1]  It's really moving beyond just playing around with AI in a lab, isn't it? This is about running it in production, integrated with real business processes.\n",
            "Generation finished in 35.62 seconds.\n",
            "\n",
            "Generating segment 14\n",
            "[S2]  Totally. That means it needs to be reliable, manageable, automatable – all the things you expect from production infrastructure. The days of just POCs are evolving fast.\n",
            "Generation finished in 49.33 seconds.\n",
            "\n",
            "Generating segment 15\n",
            "[S1]  So the key takeaway seems to be needing a really flexible, high-performance, but operationally practical data infrastructure that spans wherever AI workloads run.\n",
            "Generation finished in 38.33 seconds.\n",
            "\n",
            "Generating segment 16\n",
            "[S2]  Definitely. It needs to be ready for today's AI demands and whatever comes next, without forcing IT teams to start from scratch.\n",
            "Generation finished in 32.21 seconds.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2146, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1664, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 884, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-13-bc237aa329d1>\", line 59, in generate_podcast\n",
            "    return combine_mp3s(folder_path, folder_path + \"/finaL_podcast.mp3\")\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-8b6aadcd3617>\", line 21, in combine_mp3s\n",
            "    file_list.sort(key=extract_podcast_numbering)\n",
            "  File \"<ipython-input-10-8b6aadcd3617>\", line 10, in extract_podcast_numbering\n",
            "    return int(match.group()) if match else float('inf')\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "ValueError: invalid literal for int() with base 10: 'podcast_16.mp3'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e145f779455dee6d27.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/2025-05-04 19:07:47'\n",
        "combine_mp3s(folder_path, folder_path + \"/final_podcast.mp3\")"
      ],
      "metadata": {
        "id": "DzjgWKi0wRBf",
        "outputId": "0a11528e-f3e1-4714-f964-b5ba33421603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sort Order: ['podcast_1.mp3', 'podcast_2.mp3', 'podcast_3.mp3', 'podcast_4.mp3', 'podcast_5.mp3', 'podcast_6.mp3', 'podcast_7.mp3', 'podcast_8.mp3', 'podcast_9.mp3', 'podcast_10.mp3', 'podcast_11.mp3', 'podcast_12.mp3', 'podcast_13.mp3', 'podcast_14.mp3', 'podcast_15.mp3', 'podcast_16.mp3']\n",
            "Combined 16 MP3 files into /content/2025-05-04 19:07:47/final_podcast.mp3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/2025-05-04 19:07:47/final_podcast.mp3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}