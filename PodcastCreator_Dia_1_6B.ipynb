{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfqppHDk6ny5fulxL7uG5W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a65fda6756184e68859b08ddb61f5e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66708c16a43542cb991a9026e4206848",
              "IPY_MODEL_ada6bbcdab6e4a8e869cddc8bf6145af",
              "IPY_MODEL_b8806fc2553049adab687d666eba0067"
            ],
            "layout": "IPY_MODEL_0aac63fc34f24b02be99144e683e9af0"
          }
        },
        "66708c16a43542cb991a9026e4206848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd7e08a5d8e246b880edaf79d36f63b0",
            "placeholder": "​",
            "style": "IPY_MODEL_52256683cbf542d1a43ee4276a5119e2",
            "value": "config.json: 100%"
          }
        },
        "ada6bbcdab6e4a8e869cddc8bf6145af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e4aeb46e24e47bda5d7155a5f6312c2",
            "max": 941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b6481a15c904724a4c4f7a7a794cda8",
            "value": 941
          }
        },
        "b8806fc2553049adab687d666eba0067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a387acf18b9d45e4be8169bfd16e3b53",
            "placeholder": "​",
            "style": "IPY_MODEL_ae9180f2b036440f89a123268f481949",
            "value": " 941/941 [00:00&lt;00:00, 78.2kB/s]"
          }
        },
        "0aac63fc34f24b02be99144e683e9af0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd7e08a5d8e246b880edaf79d36f63b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52256683cbf542d1a43ee4276a5119e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e4aeb46e24e47bda5d7155a5f6312c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b6481a15c904724a4c4f7a7a794cda8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a387acf18b9d45e4be8169bfd16e3b53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9180f2b036440f89a123268f481949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "299e5f37997f4991bd6492de9c25aa7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e564ae5759f4537a69f4060519fbcf2",
              "IPY_MODEL_7b51cbe86be448559739b28406a831f7",
              "IPY_MODEL_543fc51f4c634837842456d27c155717"
            ],
            "layout": "IPY_MODEL_a219b3e2165749669f0e00344660f305"
          }
        },
        "4e564ae5759f4537a69f4060519fbcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a803e5082f4e478208b1f922eaa35f",
            "placeholder": "​",
            "style": "IPY_MODEL_4bbf633ca87243949278802b71c2ba60",
            "value": "model.safetensors: 100%"
          }
        },
        "7b51cbe86be448559739b28406a831f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063b674f196348c0bca7241d58067022",
            "max": 6444682848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ae07373d907477c91debf511468d205",
            "value": 6444682848
          }
        },
        "543fc51f4c634837842456d27c155717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0fb87101ba94fd39a185c05f1a221cc",
            "placeholder": "​",
            "style": "IPY_MODEL_71da5f13845a4060b8bdcc368ca68e5c",
            "value": " 6.44G/6.44G [00:35&lt;00:00, 278MB/s]"
          }
        },
        "a219b3e2165749669f0e00344660f305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a803e5082f4e478208b1f922eaa35f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bbf633ca87243949278802b71c2ba60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "063b674f196348c0bca7241d58067022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae07373d907477c91debf511468d205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0fb87101ba94fd39a185c05f1a221cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71da5f13845a4060b8bdcc368ca68e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cburchett/podcastcreator/blob/main/PodcastCreator_Dia_1_6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "id": "iMpzAkbKuKQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e318741-0b4e-419f-9fd8-de3c67b63b1b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2WZzjZw5JXN",
        "outputId": "138b0f40-60a4-4add-81ba-a3866971a58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nari-tts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for argbind (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for randomname (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-aiplatform 1.90.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-cloud-iam 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-cloud-dataproc 5.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.20.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.8 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.30.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.19.6 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.20.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.31.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-spanner 3.54.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install directly from GitHub\n",
        "!pip -q install git+https://github.com/nari-labs/dia.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dia.model import Dia\n",
        "\n",
        "audio_model = Dia.from_pretrained(\"nari-labs/Dia-1.6B\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "a65fda6756184e68859b08ddb61f5e80",
            "66708c16a43542cb991a9026e4206848",
            "ada6bbcdab6e4a8e869cddc8bf6145af",
            "b8806fc2553049adab687d666eba0067",
            "0aac63fc34f24b02be99144e683e9af0",
            "cd7e08a5d8e246b880edaf79d36f63b0",
            "52256683cbf542d1a43ee4276a5119e2",
            "4e4aeb46e24e47bda5d7155a5f6312c2",
            "0b6481a15c904724a4c4f7a7a794cda8",
            "a387acf18b9d45e4be8169bfd16e3b53",
            "ae9180f2b036440f89a123268f481949",
            "299e5f37997f4991bd6492de9c25aa7c",
            "4e564ae5759f4537a69f4060519fbcf2",
            "7b51cbe86be448559739b28406a831f7",
            "543fc51f4c634837842456d27c155717",
            "a219b3e2165749669f0e00344660f305",
            "17a803e5082f4e478208b1f922eaa35f",
            "4bbf633ca87243949278802b71c2ba60",
            "063b674f196348c0bca7241d58067022",
            "3ae07373d907477c91debf511468d205",
            "c0fb87101ba94fd39a185c05f1a221cc",
            "71da5f13845a4060b8bdcc368ca68e5c"
          ]
        },
        "id": "Fq-JBUlGxb5n",
        "outputId": "37d07eed-34b8-4e94-80cc-b57c67982911"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/941 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a65fda6756184e68859b08ddb61f5e80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.44G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "299e5f37997f4991bd6492de9c25aa7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "\n",
        "# Create a client\n",
        "client = genai.Client(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "iVs9W87Xbk6L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL_ID = \"gemini-2.5-pro-exp-03-25\"\n",
        "YOUTUBE_URL = \"https://www.youtube.com/watch?v=rSCaiHFRx0k\""
      ],
      "metadata": {
        "id": "Vzk684GblFZG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"Analyze the attached Youtube video.\n",
        "\n",
        "Based on the key topics, information, and events presented in the video, generate a medium length, conversational podcast script between two speakers, labeled S1 and S2.\n",
        "\n",
        "The script should summarize or discuss the main points of the video in a natural, back-and-forth dialogue format.\n",
        "\n",
        "**Crucially, format the output *exactly* as follows:**\n",
        "\n",
        "*   Each line of dialogue must start with either `[S1]` or `[S2]`.\n",
        "*   Follow the speaker tag with a space, then their dialogue.\n",
        "*   Present the dialogue turns sequentially, mimicking a conversation.\n",
        "*   Don't add any prefix or suffix to the conversation\n",
        "\n",
        "**Use this specific structure as your template:**\n",
        "\n",
        "```\n",
        "[S1] {Dialogue for speaker 1}\n",
        "[S2] {Dialogue for speaker 2}\n",
        "[S1] {Dialogue for speaker 1, potentially a reaction or follow-up}\n",
        "[S2] {Dialogue for speaker 2}\n",
        "[S1] {Dialogue for speaker 1}\n",
        "```\n",
        "\n",
        "**Example of the desired output format:**\n",
        "\n",
        "```\n",
        "[S1] Hey Sam, How are you? Let me tell you about Dia it's an open weights text to dialogue model.\n",
        "[S2] You get full control over scripts and voices.\n",
        "[S1] Wow. Amazing. (laughs)\n",
        "[S2] Try it now on Git hub or Hugging Face.\n",
        "[S1] You bet I will!\n",
        "```\n",
        "\n",
        "**Constraints:**\n",
        "\n",
        "*   Keep the turns relatively short and conversational.\n",
        "*   Focus on the core message or interesting aspects of the video.\n",
        "*   Adhere strictly to the `[S1]` / `[S2]` formatting.\n",
        "*   **Incorporate non-verbal cues where natural and appropriate.** These should be enclosed in parentheses within the dialogue line (e.g., `(laughs)` or `(sighs)`). You may use cues from this list: `(laughs)`, `(clears throat)`, `(sighs)`, `(gasps)`, `(coughs)`, `(singing)`, `(sings)`, `(mumbles)`, `(beep)`, `(groans)`, `(sniffs)`, `(claps)`, `(screams)`, `(inhales)`, `(exhales)`, `(applause)`, `(burps)`, `(humming)`, `(sneezes)`, `(whistles)`.\n",
        "*   Do not add any introductory text, explanations, or summaries outside of the formatted script itself.\n",
        "\n",
        "**Now, analyze the video and generate the script.**\n",
        "\n",
        "---\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "STB_P4fePBOE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def ensure_folder_exists():\n",
        "    now = datetime.now()\n",
        "    timestamp = now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    folder_path = '/content/' + timestamp\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Folder '{folder_path}' created.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_path}' already exists.\")\n",
        "    return folder_path"
      ],
      "metadata": {
        "id": "xKwJ6GHeneo9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "def generate_podcast_script(youtube_url, model, prompt):\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=types.Content(\n",
        "            parts=[\n",
        "                types.Part(text=prompt),\n",
        "                types.Part(\n",
        "                    file_data=types.FileData(file_uri=youtube_url)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "rYVTfmG3octh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_podcast_transcript(transcript, pairs):\n",
        "    \"\"\"Splits a podcast transcript into segments based on S1 and S2 pairs.\n",
        "\n",
        "    Args:\n",
        "        transcript: The podcast transcript as a string.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string represents a segment of the transcript.\n",
        "        Returns an empty list if the input is invalid or no valid segments are found.\n",
        "    \"\"\"\n",
        "\n",
        "    segments = []\n",
        "    try:\n",
        "        # Split the transcript into lines\n",
        "        lines = transcript.strip().split('\\n')\n",
        "\n",
        "        # Use regular expressions to find S1 and S2 pairs\n",
        "        pattern = r\"\\[(S[12])\\](.*)\"\n",
        "        s1_s2_pairs = []\n",
        "        for line in lines:\n",
        "          match = re.match(pattern, line)\n",
        "          if match:\n",
        "            s1_s2_pairs.append(match.groups())\n",
        "\n",
        "        for i in range(0, len(s1_s2_pairs), pairs):\n",
        "            segment = \"\"\n",
        "            for j in range(i, min(i + pairs, len(s1_s2_pairs))):\n",
        "                segment += f\"[{s1_s2_pairs[j][0]}] {s1_s2_pairs[j][1]}\\n\"\n",
        "            segments.append(segment.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing transcript: {e}\")\n",
        "        return []\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "QhtHdTi4q3B3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def custom_sort_key(filename):\n",
        "    # Extract the numeric part at the beginning of the filename\n",
        "    match = re.match(r'(\\d+)', filename)\n",
        "    if match:\n",
        "        number = int(match.group(1))\n",
        "        # Return a tuple where the first element is 0 for numbers 1-9 and 1 for numbers > 10\n",
        "        return (0, number) if 1 <= number <= 9 else (1, number)\n",
        "    else:\n",
        "        # If no number is found, treat it as a high priority (comes after numbers)\n",
        "        return (2,)\n",
        "\n",
        "def combine_mp3s(folder_path, output_file):\n",
        "    \"\"\"Combines all MP3 files in a folder into a single MP3 file.\n",
        "\n",
        "    Args:\n",
        "        folder_path: The path to the folder containing the MP3 files.\n",
        "        output_file: The path to the output MP3 file.\n",
        "    \"\"\"\n",
        "    combined = AudioSegment.empty()\n",
        "    file_list = os.listdir(folder_path)\n",
        "    file_list.sort(key=custom_sort_key)\n",
        "    print(\"Sort Order:\", file_list)\n",
        "    for filename in file_list:\n",
        "        if filename.endswith(\".mp3\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                segment = AudioSegment.from_mp3(filepath)\n",
        "                combined += segment\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "    combined.export(output_file, format=\"mp3\")\n",
        "    print(f\"Combined {len(file_list)} MP3 files into {output_file}\")\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "VI4Ne5VSbzti"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "JTDNMITltG4-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import torch\n",
        "import time\n",
        "\n",
        "def generate_podcast(\n",
        "      transcript: str,\n",
        "      pairs: int,\n",
        "      max_new_tokens: int,\n",
        "      cfg_scale: float,\n",
        "      temperature: float,\n",
        "      top_p: float,\n",
        "      cfg_filter_top_k: int,\n",
        "      speed_factor: float,\n",
        "      seed: int = 0,\n",
        "    ):\n",
        "\n",
        "    folder_path = ensure_folder_exists()\n",
        "\n",
        "    # Set and Display Generation Seed\n",
        "    if seed is None or seed < 0:\n",
        "        seed = random.randint(0, 2 ** 32 - 1)\n",
        "        print(f\"\\nNo seed provided, generated random seed: {seed}\\n\")\n",
        "    else:\n",
        "        print(f\"\\nUsing user-selected seed: {seed}\\n\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    segments = split_podcast_transcript(transcript, pairs)\n",
        "    print(f\"Number of segments: \" + str(len(segments)))\n",
        "\n",
        "    for idx, seg in enumerate(segments):\n",
        "      print(f\"Generating segment {idx+1}\")\n",
        "      print(seg)\n",
        "      start_time = time.time()\n",
        "      # Use torch.inference_mode() context manager for the generation call\n",
        "      with torch.inference_mode():\n",
        "        output = audio_model.generate(\n",
        "            text=seg,\n",
        "            max_tokens=max_new_tokens,\n",
        "            cfg_scale=cfg_scale,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            cfg_filter_top_k=cfg_filter_top_k\n",
        "          )\n",
        "      sf.write(folder_path + f\"/podcast_{idx+1}.mp3\", output, 44100)\n",
        "      end_time = time.time()\n",
        "      print(f\"Generation finished in {end_time - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "    return combine_mp3s(folder_path, folder_path + \"/finaL_podcast.mp3\")\n"
      ],
      "metadata": {
        "id": "Vgkn8Olhwunl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "ZaC9AnRRliXm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as podcast_script_generator:\n",
        "  gr.Markdown(\"## Podcast Generator\")\n",
        "  with gr.Tab(\"Script\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        youtube_url = gr.Textbox(label=\"URL\", value=YOUTUBE_URL)\n",
        "        selected_model = gr.Dropdown([\"gemini-2.5-pro-exp-03-25\"])\n",
        "        system_prompt = gr.Textbox(label=\"System prompt\", value=PROMPT)\n",
        "        generate_script_button = gr.Button(\"Generate script\")\n",
        "      with gr.Column():\n",
        "        podcast_script = gr.Textbox(label=\"Podcast script\", lines=20)\n",
        "  generate_script_button.click(fn=generate_podcast_script, inputs=[youtube_url, selected_model, system_prompt], outputs=[podcast_script])\n",
        "\n",
        "  with gr.Tab(\"Podcast\"):\n",
        "     with gr.Row():\n",
        "       with gr.Column():\n",
        "          final_podcast_script = gr.Textbox(label=\"Final podcast script\", lines=20)\n",
        "          pairs = gr.Slider(\n",
        "                    label=\"Segment pairs\",\n",
        "                    minimum=1,\n",
        "                    maximum=10,\n",
        "                    value=1,  # Default\n",
        "                    step=1,\n",
        "                    info=\"Higher values increase number of [S1] [S2] pairs will be in each batch.\",\n",
        "                )\n",
        "          with gr.Accordion(\"Generation Parameters\", open=False):\n",
        "                max_new_tokens = gr.Slider(\n",
        "                    label=\"Max New Tokens (Audio Length)\",\n",
        "                    minimum=860,\n",
        "                    maximum=3072,\n",
        "                    value=audio_model.config.data.audio_length,  # Use config default if available, else fallback\n",
        "                    step=50,\n",
        "                    info=\"Controls the maximum length of the generated audio (more tokens = longer audio).\",\n",
        "                )\n",
        "                cfg_scale = gr.Slider(\n",
        "                    label=\"CFG Scale (Guidance Strength)\",\n",
        "                    minimum=1.0,\n",
        "                    maximum=5.0,\n",
        "                    value=3.0,  # Default\n",
        "                    step=0.1,\n",
        "                    info=\"Higher values increase adherence to the text prompt.\",\n",
        "                )\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature (Randomness)\",\n",
        "                    minimum=1.0,\n",
        "                    maximum=1.5,\n",
        "                    value=1.1,  # Default\n",
        "                    step=0.05,\n",
        "                    info=\"Lower values make the output more deterministic, higher values increase randomness.\",\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top P (Nucleus Sampling)\",\n",
        "                    minimum=0.80,\n",
        "                    maximum=1.0,\n",
        "                    value=0.95,  # Default\n",
        "                    step=0.01,\n",
        "                    info=\"Filters vocabulary to the most likely tokens cumulatively reaching probability P.\",\n",
        "                )\n",
        "                cfg_filter_top_k = gr.Slider(\n",
        "                    label=\"CFG Filter Top K\",\n",
        "                    minimum=15,\n",
        "                    maximum=50,\n",
        "                    value=30,\n",
        "                    step=1,\n",
        "                    info=\"Top k filter for CFG guidance.\",\n",
        "                )\n",
        "                speed_factor_slider = gr.Slider(\n",
        "                    label=\"Speed Factor\",\n",
        "                    minimum=0.8,\n",
        "                    maximum=1.0,\n",
        "                    value=0.84,\n",
        "                    step=0.02,\n",
        "                    info=\"Adjusts the speed of the generated audio (1.0 = original speed).\",\n",
        "                )\n",
        "                seed_input = gr.Number(\n",
        "                    label=\"Generation Seed (Optional)\",\n",
        "                    value=-1,\n",
        "                    precision=0,  # No decimal points\n",
        "                    step=1,\n",
        "                    interactive=True,\n",
        "                    info=\"Set a generation seed for reproducible outputs. Leave empty or -1 for random seed.\",\n",
        "                )\n",
        "          generate_podcast_button = gr.Button(\"Generate podcast\")\n",
        "       with gr.Column():\n",
        "          podcast_audio = gr.Audio(label=\"Final podcast audio\", type='filepath')\n",
        "  generate_podcast_button.click(fn=generate_podcast, inputs=[\n",
        "            final_podcast_script,\n",
        "            pairs,\n",
        "            max_new_tokens,\n",
        "            cfg_scale,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            cfg_filter_top_k,\n",
        "            speed_factor_slider,\n",
        "            seed_input,\n",
        "            ], outputs=[podcast_audio])\n",
        "  podcast_script.change(fn=lambda x: x, inputs=podcast_script, outputs=final_podcast_script)\n",
        "\n",
        "podcast_script_generator.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ce_5emyBljKl",
        "outputId": "7666c250-3d62-4920-ce28-a08b2843a0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://060985c6a8baf7fc14.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://060985c6a8baf7fc14.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/2025-05-03 18:36:00' created.\n",
            "\n",
            "No seed provided, generated random seed: 915949713\n",
            "\n",
            "Number of segments: 16\n",
            "Generating segment 1\n",
            "[S1]  So, I watched this interesting discussion about AI and data infrastructure. It really highlights how AI is putting serious pressure on traditional storage, doesn't it?\n",
            "Generation finished in 48.33 seconds.\n",
            "\n",
            "Generating segment 2\n",
            "[S2]  Totally. They mentioned how these AI data pipelines need storage that can handle really varied and constantly changing workloads. It’s not your standard setup anymore.\n",
            "Generation finished in 40.80 seconds.\n",
            "\n",
            "Generating segment 3\n",
            "[S1]  Exactly. And one of the key things Krish pointed out was the need for *super* high performance, but crucially, over standard protocols like PNFS or S3.\n",
            "Generation finished in 39.45 seconds.\n",
            "\n",
            "Generating segment 4\n",
            "[S2]  Right! No one wants to rip and replace their entire network. They want to use the standard Ethernet they already have, which makes total sense for IT teams.\n",
            "Generation finished in 39.48 seconds.\n",
            "\n",
            "Generating segment 5\n",
            "[S1]  Plus, that need for independent scaling – being able to scale storage, compute, and networking separately to really maximize how you use everything.\n",
            "Generation finished in 34.65 seconds.\n",
            "\n",
            "Generating segment 6\n",
            "[S2]  Yeah, squeezing every bit out of your flash and network speeds. But it's not just about speed, is it? Russell made a good point about AI just being another workload, eventually.\n",
            "Generation finished in 35.58 seconds.\n",
            "\n",
            "Generating segment 7\n",
            "[S1]  (clears throat) That resonated. It shouldn't be this completely separate, siloed thing. IT Ops needs to manage it alongside everything else, using existing skills and processes where possible.\n",
            "Generation finished in 49.68 seconds.\n",
            "\n",
            "Generating segment 8\n",
            "[S2]  And run it in existing data centers! Krish mentioned building new data centers takes ages, like 18 months if you're lucky. (laughs) So the infrastructure needs to be energy efficient and really dense.\n",
            "Generation finished in 48.27 seconds.\n",
            "\n",
            "Generating segment 9\n",
            "[S1]  That whole \"moving from POC to production\" idea was key too. Suddenly, it's part of a real business process, and reliability becomes critical. Someone's getting paged at 2 AM if it breaks! (laughs)\n",
            "Generation finished in 48.59 seconds.\n",
            "\n",
            "Generating segment 10\n",
            "[S2]  Definitely raises the stakes. It means thinking about governance, automation, consistent APIs – all that operational stuff becomes non-negotiable.\n",
            "Generation finished in 40.29 seconds.\n",
            "\n",
            "Generating segment 11\n",
            "[S1]  And the hybrid nature of it all! Data is often moving between on-prem and the cloud, sometimes multiple clouds. You need that seamless experience.\n",
            "Generation finished in 42.25 seconds.\n",
            "\n",
            "Generating segment 12\n",
            "[S2]  Yes, breaking down those silos between environments is huge. The infrastructure in the cloud needs to feel almost native to what's on-prem. Defy that data gravity, as they put it.\n",
            "Generation finished in 48.12 seconds.\n",
            "\n",
            "Generating segment 13\n",
            "[S1]  It requires a consistent view across everything, right? Infrastructure, data plane, control plane... wherever the data or workload lives.\n",
            "Generation finished in 37.50 seconds.\n",
            "\n",
            "Generating segment 14\n",
            "[S2]  Absolutely. And being pragmatic about the cloud – using those specialized toolkits or GPUs from hyperscalers when it makes sense, without being dogmatic about being *all* in or *all* out.\n",
            "Generation finished in 43.35 seconds.\n",
            "\n",
            "Generating segment 15\n",
            "[S1]  So, the takeaway seems to be needing a flexible, high-performance, but also operationally sound and consistent data infrastructure that spans hybrid and multi-cloud environments.\n",
            "Generation finished in 45.18 seconds.\n",
            "\n",
            "Generating segment 16\n",
            "[S2]  Pretty much sums it up. It’s about future-proofing while keeping things manageable for the teams running it today.\n",
            "Generation finished in 30.15 seconds.\n",
            "\n",
            "Combined 16 MP3 files into /content/2025-05-03 18:36:00/finaL_podcast.mp3\n"
          ]
        }
      ]
    }
  ]
}