{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cburchett/podcastcreator/blob/main/PodcastCreator_Dia_1_6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iMpzAkbKuKQI",
        "outputId": "fe223612-181d-49b0-b1d2-85350a3e9a67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install gradio for the user interface\n",
        "!pip -q install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d2WZzjZw5JXN",
        "outputId": "d92e0213-44ae-45e4-ff22-f8a84e73185e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nari-tts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for argbind (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for randomname (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-dataproc 5.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.31.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-cloud-iam 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.20.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.92.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.20.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.8 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-cloud-language 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-spanner 3.54.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install directly from GitHub\n",
        "!pip -q install git+https://github.com/nari-labs/dia.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fq-JBUlGxb5n",
        "outputId": "3c9bead9-6c2d-42b5-a5c7-115d8d38a010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "62f7cea82b8b4a5ca387622d66ee44e9",
            "fd02ed2e90ac4bc5be38ac2dcf1f5996",
            "f7f28f0899c14a29b4d7e0578ce8423f",
            "d23a2d17d74b43bcbdf2b88687d1650c",
            "a37a970b23144dbba17236bb02c55ae8",
            "88440fc860f74b4ba0fcb6df93459638",
            "d11f6de9645741e4b2f1211b80775765",
            "669b1bbecd6440378e63dc3e3498d23f",
            "b86b89a1085a4814a0255522cb4a4e8a",
            "b5afa21958224f34a328e805ae438567",
            "1aced79fa2c5445e879fbf32b5f8f875",
            "47c4f27cd7ae47edaaa90b888f91bbd1",
            "f938aaa879bf4255be5fac9fc902863d",
            "12cb77f741db4127bef6e75711c61913",
            "30ae96bf2dee4056998032c2c51eba89",
            "d884ad123a324f30b4727bc257dc845b",
            "1a08a49f29244f6d899dc54cbfade81d",
            "3d940e365e8a409789e221460a2c6b7a",
            "f61c4271be834e9ca33ef31c6736075d",
            "d4d51e3fc2bb40f2b6028aab4be105d8",
            "f3d821bfac194ff3948b10faf881cef4",
            "1eed7880b0b1482d8fc1e330b0edf55f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/941 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62f7cea82b8b4a5ca387622d66ee44e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.44G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47c4f27cd7ae47edaaa90b888f91bbd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained Dia model\n",
        "from dia.model import Dia\n",
        "\n",
        "audio_model = Dia.from_pretrained(\"nari-labs/Dia-1.6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iVs9W87Xbk6L"
      },
      "outputs": [],
      "source": [
        "# Get Google API key from userdata and set it as environment variable\n",
        "\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "\n",
        "# Create a client for Google Cloud AI API\n",
        "client = genai.Client(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vzk684GblFZG"
      },
      "outputs": [],
      "source": [
        "# Define YouTube URL\n",
        "YOUTUBE_URL = \"https://www.youtube.com/watch?v=rSCaiHFRx0k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "STB_P4fePBOE"
      },
      "outputs": [],
      "source": [
        "# Define prompt for content generation\n",
        "PROMPT = \"\"\"Analyze the attached Youtube video.\n",
        "\n",
        "Based on the key topics, information, and events presented in the video, generate a medium length, conversational podcast script between two speakers, labeled S1 and S2.\n",
        "\n",
        "The script should summarize or discuss the main points of the video in a natural, back-and-forth dialogue format.\n",
        "\n",
        "**Crucially, format the output *exactly* as follows:**\n",
        "\n",
        "*   Each line of dialogue must start with either `[S1]` or `[S2]`.\n",
        "*   Follow the speaker tag with a space, then their dialogue.\n",
        "*   Present the dialogue turns sequentially, mimicking a conversation.\n",
        "*   Don't add any prefix or suffix to the conversation\n",
        "\n",
        "**Use this specific structure as your template:**\n",
        "\n",
        "```\n",
        "[S1] {Dialogue for speaker 1}\n",
        "[S2] {Dialogue for speaker 2}\n",
        "[S1] {Dialogue for speaker 1, potentially a reaction or follow-up}\n",
        "[S2] {Dialogue for speaker 2}\n",
        "[S1] {Dialogue for speaker 1}\n",
        "```\n",
        "\n",
        "**Example of the desired output format:**\n",
        "\n",
        "```\n",
        "[S1] Hey Sam, How are you? Let me tell you about Dia it's an open weights text to dialogue model.\n",
        "[S2] You get full control over scripts and voices.\n",
        "[S1] Wow. Amazing. (laughs)\n",
        "[S2] Try it now on Git hub or Hugging Face.\n",
        "[S1] You bet I will!\n",
        "```\n",
        "\n",
        "**Constraints:**\n",
        "\n",
        "*   Keep the turns relatively short and conversational.\n",
        "*   Focus on the core message or interesting aspects of the video.\n",
        "*   Adhere strictly to the `[S1]` / `[S2]` formatting.\n",
        "*   **Incorporate non-verbal cues where natural and appropriate.** These should be enclosed in parentheses within the dialogue line (e.g., `(laughs)` or `(sighs)`). You may use cues from this list: `(laughs)`, `(clears throat)`, `(sighs)`, `(gasps)`, `(coughs)`, `(singing)`, `(sings)`, `(mumbles)`, `(beep)`, `(groans)`, `(sniffs)`, `(claps)`, `(screams)`, `(inhales)`, `(exhales)`, `(applause)`, `(burps)`, `(humming)`, `(sneezes)`, `(whistles)`.\n",
        "*   Do not add any introductory text, explanations, or summaries outside of the formatted script itself.\n",
        "\n",
        "**Now, analyze the video and generate the script.**\n",
        "\n",
        "---\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FHjZV7DH6mfM",
        "outputId": "916f3ec8-40c3-4922-e7b2-f7c76585b194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTEST_SCRIPT = \"\"\"\\n[S1] So I watched this NetApp video about data infrastructure for AI, and it really highlights how demanding these new workloads are.\\n[S2] Yeah, they mentioned AI data pipelines are putting serious pressure on storage architectures, right? The requirements are constantly evolving.\\n[S1] Exactly. Especially for things like AI model training, which needs absolutely massive performance.\\n[S2] And Krish made a great point – it’s not just raw performance, but getting that performance over standard, familiar protocols like NFS or S3, using the Ethernet networks IT teams already have. (clears throat) No need to rip and replace everything.\\n[S1] That makes sense. Plus, the need for flexibility. The infrastructure has to handle these huge training jobs but also scale efficiently for inference later on.\\n[S2] And scale *independently*! They talked about scaling storage, compute, and networking separately to really maximise how resources are used.\\n[S1] That ties into what Russell was saying too, about fitting AI into the *existing* data center. (laughs) You can\\'t just build new ones overnight.\\n[S2] Right! So the underlying storage needs to be super dense and energy-efficient to work within current power and space limits. It has to integrate, not be its own isolated thing.\\n[S1] It sounds like IT Ops needs solutions that work with their current tools and skills, rather than something completely alien. AI is becoming just another workload, albeit a demanding one.\\n[S2] Absolutely. And it\\'s often hybrid – data and tools might be on-prem *and* in the cloud, maybe even multiple clouds.\\n[S1] So you need that seamless experience, right? Moving data easily, managing it consistently wherever it lives, breaking down those silos.\\n[S2] Yeah, defying \"data gravity\" as Krish put it. You need that consistent view and operation across your whole environment, on-prem and cloud(s).\\n[S1] It\\'s really moving beyond just playing around with AI in a lab, isn\\'t it? This is about running it in production, integrated with real business processes.\\n[S2] Totally. That means it needs to be reliable, manageable, automatable – all the things you expect from production infrastructure. The days of just POCs are evolving fast.\\n[S1] So the key takeaway seems to be needing a really flexible, high-performance, but operationally practical data infrastructure that spans wherever AI workloads run.\\n[S2] Definitely. It needs to be ready for today\\'s AI demands and whatever comes next, without forcing IT teams to start from scratch.\\n\"\"\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "TEST_SCRIPT = \"\"\"\n",
        "[S1] So I watched this NetApp video about data infrastructure for AI, and it really highlights how demanding these new workloads are.\n",
        "[S2] Yeah, they mentioned AI data pipelines are putting serious pressure on storage architectures, right?\n",
        "\"\"\"\n",
        "'''\n",
        "TEST_SCRIPT = \"\"\"\n",
        "[S1] So I watched this NetApp video about data infrastructure for AI, and it really highlights how demanding these new workloads are.\n",
        "[S2] Yeah, they mentioned AI data pipelines are putting serious pressure on storage architectures, right? The requirements are constantly evolving.\n",
        "[S1] Exactly. Especially for things like AI model training, which needs absolutely massive performance.\n",
        "[S2] And Krish made a great point – it’s not just raw performance, but getting that performance over standard, familiar protocols like NFS or S3, using the Ethernet networks IT teams already have. (clears throat) No need to rip and replace everything.\n",
        "[S1] That makes sense. Plus, the need for flexibility. The infrastructure has to handle these huge training jobs but also scale efficiently for inference later on.\n",
        "[S2] And scale *independently*! They talked about scaling storage, compute, and networking separately to really maximise how resources are used.\n",
        "[S1] That ties into what Russell was saying too, about fitting AI into the *existing* data center. (laughs) You can't just build new ones overnight.\n",
        "[S2] Right! So the underlying storage needs to be super dense and energy-efficient to work within current power and space limits. It has to integrate, not be its own isolated thing.\n",
        "[S1] It sounds like IT Ops needs solutions that work with their current tools and skills, rather than something completely alien. AI is becoming just another workload, albeit a demanding one.\n",
        "[S2] Absolutely. And it's often hybrid – data and tools might be on-prem *and* in the cloud, maybe even multiple clouds.\n",
        "[S1] So you need that seamless experience, right? Moving data easily, managing it consistently wherever it lives, breaking down those silos.\n",
        "[S2] Yeah, defying \"data gravity\" as Krish put it. You need that consistent view and operation across your whole environment, on-prem and cloud(s).\n",
        "[S1] It's really moving beyond just playing around with AI in a lab, isn't it? This is about running it in production, integrated with real business processes.\n",
        "[S2] Totally. That means it needs to be reliable, manageable, automatable – all the things you expect from production infrastructure. The days of just POCs are evolving fast.\n",
        "[S1] So the key takeaway seems to be needing a really flexible, high-performance, but operationally practical data infrastructure that spans wherever AI workloads run.\n",
        "[S2] Definitely. It needs to be ready for today's AI demands and whatever comes next, without forcing IT teams to start from scratch.\n",
        "\"\"\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xKwJ6GHeneo9"
      },
      "outputs": [],
      "source": [
        "# Function to create a new folder with a timestamp\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def ensure_folder_exists():\n",
        "    now = datetime.now()\n",
        "    timestamp = now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    folder_path = '/content/' + timestamp\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Folder '{folder_path}' created.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_path}' already exists.\")\n",
        "    return folder_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rYVTfmG3octh"
      },
      "outputs": [],
      "source": [
        "# Function to generate podcast script using Google Cloud AI API\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "def generate_podcast_script(youtube_url, model, prompt):\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=types.Content(\n",
        "            parts=[\n",
        "                types.Part(text=prompt),\n",
        "                types.Part(\n",
        "                    file_data=types.FileData(file_uri=youtube_url)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QhtHdTi4q3B3"
      },
      "outputs": [],
      "source": [
        "# Function to split podcast transcript into segments\n",
        "\n",
        "import re\n",
        "\n",
        "def split_podcast_transcript(transcript, pairs):\n",
        "    \"\"\"Splits a podcast transcript into segments based on S1 and S2 pairs.\n",
        "\n",
        "    Args:\n",
        "        transcript: The podcast transcript as a string.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string represents a segment of the transcript.\n",
        "        Returns an empty list if the input is invalid or no valid segments are found.\n",
        "    \"\"\"\n",
        "\n",
        "    segments = []\n",
        "    try:\n",
        "        # Split the transcript into lines\n",
        "        lines = transcript.strip().split('\\n')\n",
        "\n",
        "        # Use regular expressions to find S1 and S2 pairs\n",
        "        pattern = r\"\\[(S[12])\\](.*)\"\n",
        "        s1_s2_pairs = []\n",
        "        for line in lines:\n",
        "          match = re.match(pattern, line)\n",
        "          if match:\n",
        "            s1_s2_pairs.append(match.groups())\n",
        "\n",
        "        for i in range(0, len(s1_s2_pairs), pairs):\n",
        "            segment = \"\"\n",
        "            for j in range(i, min(i + pairs, len(s1_s2_pairs))):\n",
        "                segment += f\"[{s1_s2_pairs[j][0]}] {s1_s2_pairs[j][1]}\\n\"\n",
        "            segments.append(segment.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing transcript: {e}\")\n",
        "        return []\n",
        "\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VI4Ne5VSbzti"
      },
      "outputs": [],
      "source": [
        "# Function to extract podcast numbering from filename\n",
        "# Function to combine multiple MP3 files into one\n",
        "import os\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def extract_podcast_numbering(filename):\n",
        "    # Extract the numeric part at the beginning of the filename\n",
        "    match = re.match(r'podcast_(\\d+)\\.mp3', filename)\n",
        "    return int(match.group(1)) if match else float('inf')\n",
        "\n",
        "def combine_mp3s(folder_path, output_file):\n",
        "    \"\"\"Combines all MP3 files in a folder into a single MP3 file.\n",
        "\n",
        "    Args:\n",
        "        folder_path: The path to the folder containing the MP3 files.\n",
        "        output_file: The path to the output MP3 file.\n",
        "    \"\"\"\n",
        "    combined = AudioSegment.empty()\n",
        "    file_list = [f for f in os.listdir(folder_path) if f.endswith(\".mp3\")]\n",
        "    file_list.sort(key=extract_podcast_numbering)\n",
        "    print(\"Sort Order:\", file_list)\n",
        "    for filename in file_list:\n",
        "        if filename.endswith(\".mp3\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                segment = AudioSegment.from_mp3(filepath)\n",
        "                combined += segment\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "    combined.export(output_file, format=\"mp3\")\n",
        "    print(f\"Combined {len(file_list)} MP3 files into {output_file}\")\n",
        "    return output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BQ1roOdi1pbE"
      },
      "outputs": [],
      "source": [
        "def write_formatted_script_to_file(script_content, output_file_path):\n",
        "    \"\"\"\n",
        "    Writes a formatted Python script to a file for clear reading.\n",
        "\n",
        "    Args:\n",
        "        script_content (str): The content of the Python script as a string.\n",
        "        output_file_path (str): The path to the output file where the script will be written.\n",
        "    \"\"\"\n",
        "    # Split the script content into lines\n",
        "    lines = script_content.split('\\n')\n",
        "\n",
        "    # Format the lines for clear reading\n",
        "    formatted_lines = []\n",
        "    for line in lines:\n",
        "        # Strip leading and trailing whitespace\n",
        "        stripped_line = line.strip()\n",
        "        # Add the formatted line to the list, ensuring proper indentation\n",
        "        if stripped_line:\n",
        "            formatted_lines.append(stripped_line)\n",
        "\n",
        "    # Join the formatted lines back into a single string with proper line breaks\n",
        "    formatted_script = '\\n'.join(formatted_lines)\n",
        "\n",
        "    # Write the formatted script to the output file\n",
        "    with open(output_file_path, 'w') as file:\n",
        "        file.write(formatted_script)\n",
        "\n",
        "    print(f\"Formatted script has been written to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JTDNMITltG4-"
      },
      "outputs": [],
      "source": [
        "# Function to set random seed for reproducibility\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mGoiTw5t6mfO"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def generate_podcast_segment(\n",
        "      seg: str,\n",
        "      idx: int,\n",
        "      max_new_tokens: int,\n",
        "      cfg_scale: float,\n",
        "      temperature: float,\n",
        "      top_p: float,\n",
        "      cfg_filter_top_k: int,\n",
        "      folder_path: str\n",
        "    ):\n",
        "\n",
        "    print(f\"Generating segment {idx+1}\")\n",
        "    print(seg)\n",
        "    start_time = time.time()\n",
        "    # Use torch.inference_mode() context manager for the generation call\n",
        "    with torch.inference_mode():\n",
        "      output = audio_model.generate(\n",
        "          text=seg,\n",
        "          max_tokens=max_new_tokens,\n",
        "          cfg_scale=cfg_scale,\n",
        "          temperature=temperature,\n",
        "          top_p=top_p,\n",
        "          cfg_filter_top_k=cfg_filter_top_k\n",
        "        )\n",
        "    sf.write(folder_path + f\"/podcast_{idx+1}.mp3\", output, 44100)\n",
        "    end_time = time.time()\n",
        "    print(f\"Generation finished in {end_time - start_time:.2f} seconds.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vgkn8Olhwunl"
      },
      "outputs": [],
      "source": [
        "# Function to generate podcast audio\n",
        "\n",
        "import soundfile as sf\n",
        "\n",
        "def generate_podcast_segments(\n",
        "      transcript: str,\n",
        "      pairs: int,\n",
        "      max_new_tokens: int,\n",
        "      cfg_scale: float,\n",
        "      temperature: float,\n",
        "      top_p: float,\n",
        "      cfg_filter_top_k: int,\n",
        "      speed_factor: float,\n",
        "      folder_path: str,\n",
        "      seed: int = 0,\n",
        "    ):\n",
        "\n",
        "    # Set and Display Generation Seed\n",
        "    if seed is None or seed < 0:\n",
        "        seed = random.randint(0, 2 ** 32 - 1)\n",
        "        print(f\"\\nNo seed provided, generated random seed: {seed}\\n\")\n",
        "    else:\n",
        "        print(f\"\\nUsing user-selected seed: {seed}\\n\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Split transcript into segments\n",
        "    segments = split_podcast_transcript(transcript, pairs)\n",
        "    print(f\"Number of segments: \" + str(len(segments)))\n",
        "\n",
        "    output_data = [] # List to store tuples of (segment text, audio path)\n",
        "\n",
        "    # Generate audio for each segment\n",
        "    for idx, seg in enumerate(segments):\n",
        "      generate_podcast_segment(seg, idx,\n",
        "                               max_new_tokens,\n",
        "                               cfg_scale,\n",
        "                               temperature,\n",
        "                               top_p,\n",
        "                               cfg_filter_top_k,\n",
        "                               folder_path)\n",
        "\n",
        "\n",
        "      audio_path = folder_path + f\"/podcast_{idx+1}.mp3\"\n",
        "\n",
        "    output_data.append((seg, audio_path)) # Append segment text and audio path\n",
        "\n",
        "    return output_data # Return the list of data and the folder path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rbWt2bRN6mfP"
      },
      "outputs": [],
      "source": [
        "def generate_final_podcast(\n",
        "        FOLDER_PATH: str,\n",
        "        transcript: str\n",
        "        ):\n",
        "\n",
        "        # Print out transcript\n",
        "        output_file_path = FOLDER_PATH + \"/final_transcript.txt\"\n",
        "        write_formatted_script_to_file(transcript, output_file_path)\n",
        "        print(f\"Transcript written to {output_file_path}\")\n",
        "\n",
        "        # Combine audio segments into final podcast\n",
        "        return combine_mp3s(FOLDER_PATH, FOLDER_PATH + \"/final_podcast.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ce_5emyBljKl",
        "outputId": "2f6bffbd-143d-4086-d076-971809acd51f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1fbe4268e5a990fc11.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1fbe4268e5a990fc11.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/2025-05-25 16:22:42' created.\n",
            "\n",
            "No seed provided, generated random seed: 3390997336\n",
            "\n",
            "Number of segments: 2\n",
            "Generating segment 1\n",
            "[S1]  So I watched this NetApp video about data infrastructure for AI, and it really highlights how demanding these new workloads are.\n",
            "Generation finished in 53.05 seconds.\n",
            "\n",
            "Generating segment 2\n",
            "[S2]  Yeah, they mentioned AI data pipelines are putting serious pressure on storage architectures, right?\n",
            "Generation finished in 25.88 seconds.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2201, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1978, in postprocess_data\n",
            "    raise InvalidComponentError(\n",
            "gradio.exceptions.InvalidComponentError: <class 'gradio.layouts.column.Column'> Component not a valid output component.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1fbe4268e5a990fc11.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Create Gradio interface for podcast generation\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "with gr.Blocks() as podcast_script_generator:\n",
        "  segments_state = gr.State([])\n",
        "  gr.Markdown(\"## Podcast Generator\")\n",
        "  with gr.Tab(\"Script\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        youtube_url = gr.Textbox(label=\"URL\", value=YOUTUBE_URL)\n",
        "        selected_model = gr.Dropdown([\"gemini-2.5-pro-exp-03-25\"], label=\"Transcript model\")\n",
        "        system_prompt = gr.Textbox(label=\"System prompt\", value=PROMPT)\n",
        "        generate_script_button = gr.Button(\"Generate script\")\n",
        "      with gr.Column():\n",
        "        podcast_script = gr.Textbox(label=\"Podcast script\", lines=20, value=TEST_SCRIPT)  # Remove val when move to production\n",
        "  generate_script_button.click(fn=generate_podcast_script, inputs=[youtube_url, selected_model, system_prompt], outputs=[podcast_script])\n",
        "\n",
        "  with gr.Tab(\"Podcast\"):\n",
        "     with gr.Row():\n",
        "       with gr.Column():\n",
        "          final_podcast_script = gr.Textbox(label=\"Final podcast script\", lines=20, value=TEST_SCRIPT) # Remove val when move to production\n",
        "          pairs = gr.Slider(\n",
        "                    label=\"Segment pairs\",\n",
        "                    minimum=1,\n",
        "                    maximum=10,\n",
        "                    value=1,  # Default\n",
        "                    step=1,\n",
        "                    info=\"Higher values increase number of [S1] [S2] pairs will be in each batch.\",\n",
        "                )\n",
        "          with gr.Accordion(\"Generation Parameters\", open=False):\n",
        "                max_new_tokens = gr.Slider(\n",
        "                    label=\"Max New Tokens (Audio Length)\",\n",
        "                    minimum=860,\n",
        "                    maximum=3072,\n",
        "                    value=audio_model.config.data.audio_length,  # Use config default if available, else fallback\n",
        "                    step=50,\n",
        "                    info=\"Controls the maximum length of the generated audio (more tokens = longer audio).\",\n",
        "                )\n",
        "                cfg_scale = gr.Slider(\n",
        "                    label=\"CFG Scale (Guidance Strength)\",\n",
        "                    minimum=1.0,\n",
        "                    maximum=5.0,\n",
        "                    value=3.0,  # Default\n",
        "                    step=0.1,\n",
        "                    info=\"Higher values increase adherence to the text prompt.\",\n",
        "                )\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature (Randomness)\",\n",
        "                    minimum=1.0,\n",
        "                    maximum=1.5,\n",
        "                    value=1.1,  # Default\n",
        "                    step=0.05,\n",
        "                    info=\"Lower values make the output more deterministic, higher values increase randomness.\",\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top P (Nucleus Sampling)\",\n",
        "                    minimum=0.80,\n",
        "                    maximum=1.0,\n",
        "                    value=0.95,  # Default\n",
        "                    step=0.01,\n",
        "                    info=\"Filters vocabulary to the most likely tokens cumulatively reaching probability P.\",\n",
        "                )\n",
        "                cfg_filter_top_k = gr.Slider(\n",
        "                    label=\"CFG Filter Top K\",\n",
        "                    minimum=15,\n",
        "                    maximum=50,\n",
        "                    value=30,\n",
        "                    step=1,\n",
        "                    info=\"Top k filter for CFG guidance.\",\n",
        "                )\n",
        "                speed_factor_slider = gr.Slider(\n",
        "                    label=\"Speed Factor\",\n",
        "                    minimum=0.8,\n",
        "                    maximum=1.0,\n",
        "                    value=0.84,\n",
        "                    step=0.02,\n",
        "                    info=\"Adjusts the speed of the generated audio (1.0 = original speed).\",\n",
        "                )\n",
        "                seed_input = gr.Number(\n",
        "                    label=\"Generation Seed (Optional)\",\n",
        "                    value=-1,\n",
        "                    precision=0,  # No decimal points\n",
        "                    step=1,\n",
        "                    interactive=True,\n",
        "                    info=\"Set a generation seed for reproducible outputs. Leave empty or -1 for random seed.\",\n",
        "                )\n",
        "          with gr.Row():\n",
        "            generated_filepath = gr.Textbox(label=\"File Path\")\n",
        "            generate_path_button = gr.Button(\"Generate path\")\n",
        "          generate_podcast_segments_button = gr.Button(\"Generate podcast segments\")\n",
        "       with gr.Column():\n",
        "\n",
        "         with gr.Accordion(\"Segments\", open=False) as all_segments_accordion:\n",
        "             segments_container = gr.Column()\n",
        "\n",
        "         generate_final_podcast_button = gr.Button(\"Generate final podcast\")\n",
        "         final_podcast_audio = gr.Audio(label=\"Final podcast audio\", type='filepath')\n",
        "\n",
        "  generate_path_button.click(fn=ensure_folder_exists, outputs=[generated_filepath])\n",
        "\n",
        "  def update_segments_in_accordion(output_data):\n",
        "    # This function creates the components to display inside the accordion\n",
        "    components = []\n",
        "    for i, (text, audio_path) in enumerate(output_data):\n",
        "        components.append(gr.Textbox(label=f\"Segment {i+1} Text\", value=text, interactive=False))\n",
        "        components.append(gr.Audio(label=f\"Segment {i+1} Audio\", value=audio_path, type='filepath'))\n",
        "    return components # Return a list of components\n",
        "\n",
        "  generate_podcast_segments_button.click(\n",
        "      fn=generate_podcast_segments, # Call the generation function and pass its output\n",
        "      inputs=[\n",
        "            final_podcast_script,\n",
        "            pairs,\n",
        "            max_new_tokens,\n",
        "            cfg_scale,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            cfg_filter_top_k,\n",
        "            speed_factor_slider,\n",
        "            generated_filepath,\n",
        "            seed_input,\n",
        "      ],\n",
        "      outputs=[segments_state]\n",
        "  ).then(\n",
        "        fn=update_segments_in_accordion, # Call the update function with the generation output\n",
        "        inputs=[segments_state], # Get the output from the previous function call\n",
        "        outputs=[segments_container] # Update the segments container\n",
        "  )\n",
        "\n",
        "  generate_final_podcast_button.click(fn=generate_final_podcast, inputs=[\n",
        "     generated_filepath,\n",
        "     final_podcast_script\n",
        "  ], outputs=[final_podcast_audio])\n",
        "\n",
        "  # Used to auto-update the final_podcast_script textbox with the value generated on the previous tab\n",
        "  podcast_script.change(fn=lambda x: x, inputs=podcast_script, outputs=final_podcast_script)\n",
        "\n",
        "podcast_script_generator.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kd32RlyquV7Z"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62f7cea82b8b4a5ca387622d66ee44e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd02ed2e90ac4bc5be38ac2dcf1f5996",
              "IPY_MODEL_f7f28f0899c14a29b4d7e0578ce8423f",
              "IPY_MODEL_d23a2d17d74b43bcbdf2b88687d1650c"
            ],
            "layout": "IPY_MODEL_a37a970b23144dbba17236bb02c55ae8"
          }
        },
        "fd02ed2e90ac4bc5be38ac2dcf1f5996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88440fc860f74b4ba0fcb6df93459638",
            "placeholder": "​",
            "style": "IPY_MODEL_d11f6de9645741e4b2f1211b80775765",
            "value": "config.json: 100%"
          }
        },
        "f7f28f0899c14a29b4d7e0578ce8423f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669b1bbecd6440378e63dc3e3498d23f",
            "max": 941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b86b89a1085a4814a0255522cb4a4e8a",
            "value": 941
          }
        },
        "d23a2d17d74b43bcbdf2b88687d1650c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5afa21958224f34a328e805ae438567",
            "placeholder": "​",
            "style": "IPY_MODEL_1aced79fa2c5445e879fbf32b5f8f875",
            "value": " 941/941 [00:00&lt;00:00, 36.8kB/s]"
          }
        },
        "a37a970b23144dbba17236bb02c55ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88440fc860f74b4ba0fcb6df93459638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d11f6de9645741e4b2f1211b80775765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "669b1bbecd6440378e63dc3e3498d23f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b86b89a1085a4814a0255522cb4a4e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5afa21958224f34a328e805ae438567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aced79fa2c5445e879fbf32b5f8f875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47c4f27cd7ae47edaaa90b888f91bbd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f938aaa879bf4255be5fac9fc902863d",
              "IPY_MODEL_12cb77f741db4127bef6e75711c61913",
              "IPY_MODEL_30ae96bf2dee4056998032c2c51eba89"
            ],
            "layout": "IPY_MODEL_d884ad123a324f30b4727bc257dc845b"
          }
        },
        "f938aaa879bf4255be5fac9fc902863d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a08a49f29244f6d899dc54cbfade81d",
            "placeholder": "​",
            "style": "IPY_MODEL_3d940e365e8a409789e221460a2c6b7a",
            "value": "model.safetensors: 100%"
          }
        },
        "12cb77f741db4127bef6e75711c61913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f61c4271be834e9ca33ef31c6736075d",
            "max": 6444682848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4d51e3fc2bb40f2b6028aab4be105d8",
            "value": 6444682848
          }
        },
        "30ae96bf2dee4056998032c2c51eba89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3d821bfac194ff3948b10faf881cef4",
            "placeholder": "​",
            "style": "IPY_MODEL_1eed7880b0b1482d8fc1e330b0edf55f",
            "value": " 6.44G/6.44G [00:37&lt;00:00, 296MB/s]"
          }
        },
        "d884ad123a324f30b4727bc257dc845b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a08a49f29244f6d899dc54cbfade81d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d940e365e8a409789e221460a2c6b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f61c4271be834e9ca33ef31c6736075d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d51e3fc2bb40f2b6028aab4be105d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3d821bfac194ff3948b10faf881cef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eed7880b0b1482d8fc1e330b0edf55f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}